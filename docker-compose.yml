version: "3.0"
services:
  proxy:
    build: ./docker/proxy/
    image: mcleonte/zaida-proxy
    container_name: zaida-proxy
    ports:
      - ${NLU_PORT}:${NLU_PORT}
      - ${STT_PORT}:${STT_PORT}
      - ${TTS_PORT}:${TTS_PORT}
    # depends_on:
    #   - stt
    #   - nlu
    #   - tts

  nlu:
    build: ./docker/nlu/
    image: mcleonte/zaida-nlu
    container_name: zaida-nlu
    user: 1000:1000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://nlu:${NLU_PORT}"]
      interval: 5s
      retries: 20
    expose:
      - ${NLU_PORT}
    volumes:
      - ./docker/nlu:/app
      - spacy_models:/opt/venv/lib/python3.10/site-packages/en_core_web_lg
    command: run --debug --port ${NLU_PORT}
    environment:
      - SQLALCHEMY_SILENCE_UBER_WARNING=1
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              count: 1

  nlu-actions:
    build: ./docker/nlu-actions/
    image: mcleonte/zaida-nlu-actions
    container_name: zaida-nlu-actions
    expose:
      - 8000
    volumes:
      - ./docker/nlu-actions/actions:/app/actions
      - cache_huggingface_hub:/app/.cache/huggingface/hub
      - /etc/localtime:/etc/localtime
    command: --port 8000
    environment:
      - TRANSFORMERS_CACHE=/app/cache/
      - LOG_LEVEL=${LOG_LEVEL}

  stt:
    build: ./docker/stt/
    image: mcleonte/zaida-stt
    container_name: zaida-stt
    expose:
      - ${STT_PORT}
    volumes:
      - ./docker/stt/:/app/
    environment:
      - PROXY_PORT=${STT_PORT}
      - NLU_HOST=nlu
      - NLU_PORT=${NLU_PORT}
      - LOG_LEVEL=${LOG_LEVEL}
      - MODEL_PATH=/app/models
      - MODEL_NAME=small.en
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              count: 1
    # depends_on:
    #   nlu:
    #     condition: service_healthy

  tts:
    build:
      context: ./docker/tts
      dockerfile: gpu.Dockerfile
    image: mcleonte/zaida-tts
    container_name: zaida-tts
    expose:
      - ${NLU_PORT}
      - ${TTS_PORT}
    volumes:
      - mimic3_voices:/home/mimic3/.local/share/mycroft/mimic3/voices/
    environment:
      - PROXY_PORT=${TTS_PORT}
      - LOG_LEVEL=${LOG_LEVEL}
    command: --preload-voice en_US/vctk_low --voice en_US/vctk_low#p329 --debug --port ${NLU_PORT} --cuda
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              count: 1

volumes:
  cache_huggingface_hub:
  mimic3_voices:
  spacy_models:
